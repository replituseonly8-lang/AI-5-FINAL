import requests
import json
import re
import config
from utils import AnimatedLoader

# Global conversation memory
conversation_memory = {}

def _append_delta_text_from_chunk(obj, buf):
    """
    Safely extract streamed text from an OpenAI-compatible SSE JSON object.
    Handles shapes:
      - {"choices":[{"delta":{"content":"..."} }]}
      - {"choices":[{"delta":{"role":"assistant"}}]}  # no content
      - {"choices":[{"message":{"content":"..."} }]}  # non-stream JSON fallback
    Also tolerates when 'choices' is a list but not dict-like.
    """
    try:
        choices = obj.get("choices", [])
    except AttributeError:
        # obj itself might already be a list of choices
        choices = obj if isinstance(obj, list) else []
    for choice in choices:
        # choice may be dict; guard everything
        if not isinstance(choice, dict):
            continue
        delta = choice.get("delta") or {}
        if isinstance(delta, dict):
            piece = delta.get("content")
            if isinstance(piece, str):
                buf.append(piece)
        # Some proxies send non-stream JSON in SSE pipe
        message = choice.get("message") or {}
        if isinstance(message, dict):
            content = message.get("content")
            if isinstance(content, str):
                buf.append(content)

def parse_streaming_response(response):
    """Robust SSE parser tolerant to proxies and concatenated or array chunks."""
    out_parts = []
    try:
        for raw in response.iter_lines(decode_unicode=True):
            if not raw or raw.startswith(":"):
                continue
            if raw.startswith("data:"):
                data = raw[5:].lstrip()
            else:
                continue
            if not data or data == "[DONE]":
                continue

            # Split concatenated JSON objects if "}{" seam appears
            pieces = re.split(r'(?<=\})(?=\{)', data) if "}{" in data else [data]
            for p in pieces:
                p = p.strip()
                if not p:
                    continue
                try:
                    obj = json.loads(p)
                except json.JSONDecodeError:
                    # Some proxies batch choices as a JSON array; try to wrap per-line arrays
                    if p.startswith("[") and p.endswith("]"):
                        try:
                            arr = json.loads(p)
                            _append_delta_text_from_chunk({"choices": arr}, out_parts)
                            continue
                        except Exception:
                            out_parts.append(p)
                            continue
                    # As last resort append raw text
                    out_parts.append(p)
                    continue
                _append_delta_text_from_chunk(obj, out_parts)
        return "".join(out_parts).strip()
    except Exception as e:
        print(f"[DEBUG] Streaming parse error: {e}")
        return None

def get_ai_response(user_message, user_name="User", chat_id=None, message_context=None):
    """Get AI response with streaming support and conversation memory"""
    result = ""
    current_message = f"{user_name}: {user_message}"

    try:
        # Build conversation context
        messages = [
            {"role": "system", "content": config.SYSTEM_PROMPT}
        ]

        # Add conversation memory if available
        if chat_id in conversation_memory:
            messages.extend(conversation_memory[chat_id][-6:])  # Keep last 6 messages for context

        # Add current message with context
        if message_context:
            current_message = f"[Context: {message_context}] {current_message}"

        messages.append({"role": "user", "content": current_message})

        headers = {"Content-Type": "application/json"}
        payload = {
            "model": config.CHAT_MODEL,
            "messages": messages,
            "max_tokens": 1000,
            "temperature": 0.8,
            "stream": True
        }

        print(f"[DEBUG] Sending request to: {config.CHAT_API_ENDPOINT}")
        response = requests.post(
            config.CHAT_API_ENDPOINT,
            json=payload,
            headers=headers,
            stream=True,
            timeout=60
        )

        response.raise_for_status()
        content_type = response.headers.get('Content-Type', '').lower().strip()

        if "text/event-stream" in content_type or content_type == "" or "event-stream" in content_type:
            ai_response = parse_streaming_response(response)
            result = ai_response if ai_response else "ðŸ”„ **Streaming Error:** Unable to parse response."
        elif "application/json" in content_type:
            data = response.json()
            # Non-streaming JSON format
            try:
                if "choices" in data and data["choices"]:
                    choice0 = data["choices"]
                    msg = choice0.get("message", {})
                    result = (msg.get("content") or "").strip() or "ðŸ” **Response Error:** Empty content."
                else:
                    result = "ðŸ” **Response Error:** Invalid response structure."
            except Exception as e:
                result = f"ðŸ” **Response Error:** {e}"
        else:
            # Try SSE parsing anyway if mislabeled
            ai_response = parse_streaming_response(response)
            result = ai_response if ai_response else f"ðŸš¨ **API Error:** Unexpected content type: {content_type}"

    except requests.exceptions.HTTPError as http_err:
        result = f"ðŸž **HTTP Error:** {http_err}"
    except requests.exceptions.ConnectionError:
        result = "ðŸ”Œ **Connection Error:** Unable to reach API endpoint."
    except requests.exceptions.Timeout:
        result = "â³ **Timeout Error:** API response took too long."
    except Exception as ex:
        result = f"ðŸ’¥ **Error:** {str(ex)[:100]}..."

    # Store conversation in memory
    if chat_id and result:
        if chat_id not in conversation_memory:
            conversation_memory[chat_id] = []
        conversation_memory[chat_id].append({"role": "user", "content": current_message})
        conversation_memory[chat_id].append({"role": "assistant", "content": result})
        # Keep only last 10 messages to prevent memory overload
        if len(conversation_memory[chat_id]) > 10:
            conversation_memory[chat_id] = conversation_memory[chat_id][-10:]
    return result

def handle_chat_message(bot, message, chat_mode_users, user_waiting_for_chat):
    """Handle chat messages in chat mode with memory"""
    from utils import log_user_interaction, get_user_mention

    user_id = message.from_user.id
    user_name = message.from_user.first_name or "User"

    # Log interaction
    log_user_interaction(message.from_user, "chat", "DM" if message.chat.type == "private" else "Group")

    if user_id in user_waiting_for_chat:
        user_waiting_for_chat.remove(user_id)

    # Prepare message context
    context = None
    if message.reply_to_message:
        context = "Replying to previous message"
    elif message.chat.type in ['group', 'supergroup']:
        context = "Group conversation"

    # Get AI response with conversation memory
    ai_response = get_ai_response(message.text, user_name, message.chat.id, context)

    # Send the response
    try:
        bot.send_message(message.chat.id, ai_response, parse_mode="Markdown")
    except Exception as e:
        print(f"[DEBUG] Failed to send chat response: {e}")
        bot.send_message(message.chat.id, ai_response)

def handle_prompt_command(bot, message):
    """Handle /prompt command for enhancing prompts with animation"""
    from utils import log_user_interaction, AnimatedLoader

    log_user_interaction(message.from_user, "/prompt", "DM" if message.chat.type == "private" else "Group")

    prompt_text = message.text.strip()

    if len(prompt_text.split()) <= 1:
        bot.reply_to(message, """â“ **Prompt Enhancement Help**

**Usage:** `/prompt [your text]`

**Examples:**
â€¢ `/prompt a warrior` â†’ Enhanced warrior description
â€¢ `/prompt sunset landscape` â†’ Detailed scenic prompt
â€¢ `/prompt explain quantum physics` â†’ Structured explanation

**ðŸ’¡ This command enhances ideas with rich details for chat or image generation!**""", parse_mode="Markdown")
        return

    original_prompt = prompt_text[7:].strip()  # Remove "/prompt "

    # Friendâ€™s photorealistic-first update: enforce realism unless anime/cartoon requested
    wants_style = any(s in original_prompt.lower() for s in ["anime", "cartoon", "toon", "manga"])
    realism_block = (
        "Photorealistic, ultra-detailed cinematic photograph of {idea} in a real setting; environment, time of day, "
        "weather, mood, background; physically accurate lighting (key/fill/rim), soft shadows, reflections, "
        "volumetric light; camera: full-frame, 35/50/85mm prime, aperture f/1.8â€“f/4, shallow depth of field, HDR; "
        "composition with leading lines/foreground depth; micro-textures (skin pores, fabric weave, dust motes), "
        "natural color grade, subtle film grain; must read as a real photo at normal resolution. "
        "Negative: cartoon, anime, illustration, CGI, 3D render, plastic skin, oversmooth, lowres, blurry, watermark, text."
    ).format(idea=original_prompt)

    friend_hint = (
        "If the idea explicitly mentions anime/cartoon, produce one rich prompt in that style; otherwise enforce strict "
        "photorealism with camera/lens, lighting physics, composition, and microâ€‘textures. Output ONLY the final prompt."
    )

    enhanced_prompt = f"""
You are a Prompt Generator for Image Generation AI.
Rewrite the user's idea into a vivid, cinematic description.

Focus on:
- Maximum realism and intricate details
- Photorealistic textures
- Lighting and shadows
- Mood and atmosphere
- Depth of field and focus
- Ambient, background details

Do not add style labels like cartoon/anime unless explicitly requested by the idea.
Always aim for masterpiece quality and 2K resolution.
If cartoonish or any anime is not mentioned please dont give it.
Always look for default quality and look realistic.

Now rewrite this prompt and ONLY output the final enhanced result:

{(original_prompt if wants_style else realism_block)}

# Friend guidance (do not include in output):
{friend_hint}
"""

    user_name = message.from_user.first_name or "User"

    # Start animated loading for prompt enhancement
    loader = AnimatedLoader(bot, message.chat.id, "Enhancing prompt", "prompt")
    loader.start()

    try:
        # Get enhanced response
        enhanced = get_ai_response(enhanced_prompt, user_name, message.chat.id)

        # Stop loader
        loader.stop()

        response = f"âœ¨ **Enhanced Prompt:**\n\n`{enhanced}`\n\nðŸ’¡ *Copy the text above for better AI results!*"
        try:
            bot.reply_to(message, response, parse_mode="Markdown")
        except Exception as e:
            print(f"[DEBUG] Failed to send enhanced prompt: {e}")
            bot.reply_to(message, response)
    except Exception as e:
        loader.stop()
        bot.reply_to(message, f"âŒ Error enhancing prompt: {str(e)[:100]}...")
